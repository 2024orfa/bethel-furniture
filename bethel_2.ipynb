{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch https://bethelfurniture.com/store/: 404 Client Error: Not Found for url: https://bethelfurniture.com/store/\n",
      "Content successfully scraped and saved to recursive_extracted_content.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n\\n', text)\n",
    "    text = re.sub(r'(info@bethelfurniture\\.com\\s+)+', r'info@bethelfurniture.com\\n', text)\n",
    "    return text\n",
    "\n",
    "def extract_links(soup, base_url):\n",
    "    \"\"\"\n",
    "    Extract all relevant page links from a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    links = set()\n",
    "    for anchor in soup.find_all(\"a\", href=True):\n",
    "        href = anchor['href']\n",
    "        if href.startswith('/'):\n",
    "            href = base_url + href\n",
    "        if href.startswith(base_url):\n",
    "            links.add(href)\n",
    "    return links\n",
    "\n",
    "def fetch_and_clean_page(url, headers):\n",
    "    \"\"\"\n",
    "    Fetch a page, parse its content, and clean the text.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, 'lxml')  # Changed parser to 'lxml'\n",
    "    \n",
    "    # Extract text content\n",
    "    for script in soup([\"script\", \"style\", \"video\", \"img\"]):\n",
    "        script.decompose()\n",
    "    text_content = soup.get_text(separator=\"\\n\").strip()\n",
    "    text_content = clean_text(text_content)\n",
    "    \n",
    "    return text_content, soup\n",
    "\n",
    "# Function to read WhatsApp chat files and return combined content\n",
    "def read_whatsapp_chats(chat_files):\n",
    "    whatsapp_content = []\n",
    "    for chat_file in chat_files:\n",
    "        try:\n",
    "            with open(chat_file, 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                whatsapp_content.append(f\"Content from {chat_file}:\\n\" + \"=\"*20 + '\\n\\n' + content + '\\n\\n')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {chat_file} not found. Skipping...\")\n",
    "    return \"\\n\".join(whatsapp_content)\n",
    "\n",
    "# Main URL to start scraping\n",
    "base_url = \"https://bethelfurniture.com\"\n",
    "\n",
    "# Set to keep track of visited URLs\n",
    "visited_urls = set()\n",
    "\n",
    "# List to gather all content\n",
    "all_content = []\n",
    "\n",
    "# Define headers to mimic a web browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}\n",
    "\n",
    "# Queue for URLs to visit\n",
    "urls_to_visit = {base_url}\n",
    "\n",
    "while urls_to_visit:\n",
    "    current_url = urls_to_visit.pop()\n",
    "    if current_url in visited_urls:\n",
    "        continue\n",
    "    visited_urls.add(current_url)\n",
    "\n",
    "    try:\n",
    "        page_content, page_soup = fetch_and_clean_page(current_url, headers)\n",
    "        all_content.append(f\"Content from {current_url}:\\n\" + \"=\"*20 + '\\n\\n' + page_content + '\\n\\n')\n",
    "        \n",
    "        # Extract new links and add to the queue\n",
    "        new_links = extract_links(page_soup, base_url)\n",
    "        urls_to_visit.update(new_links - visited_urls)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "# Combine all the web scraping content\n",
    "combined_content = \"\\n\".join(all_content)\n",
    "\n",
    "# WhatsApp chat files\n",
    "whatsapp_chat_files = [\"WhatsApp Chat 1.txt\", \"WhatsApp Chat 2.txt\", \"WhatsApp Chat 3.txt\", \"WhatsApp Chat 4.txt\", \"WhatsApp Chat 5.txt\"]\n",
    "\n",
    "# Append WhatsApp chats content to the scraped content\n",
    "whatsapp_content = read_whatsapp_chats(whatsapp_chat_files)\n",
    "combined_content += \"\\n\" + whatsapp_content\n",
    "\n",
    "# Define the path to save the combined content\n",
    "file_path = \"recursive_extracted_content.txt\"\n",
    "\n",
    "# Write the combined text to a file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(combined_content)\n",
    "\n",
    "print(f\"Content successfully scraped and saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lynnmompremier/Desktop/Edwyn/Bethel/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize Pinecone with PineconeClient\n",
    "client = Pinecone(api_key=\"0990c15d-15bf-4dd8-80af-2361a2df1aa3\", environment=\"us-east-1\")\n",
    "# Define the index name\n",
    "index_name = \"bethel\"\n",
    "# Check if the index already exists, if not, create it\n",
    "if index_name not in client.list_indexes().names():\n",
    "    client.create_index(\n",
    "        index_name, \n",
    "        dimension=768, \n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize Pinecone with PineconeClient\n",
    "client = Pinecone(api_key=\"0990c15d-15bf-4dd8-80af-2361a2df1aa3\", environment=\"us-east-1\")\n",
    "# Define the index namefrom pinecone import Pinecone\n",
    "client.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
